{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbzBJ1m9FBBb"
      },
      "source": [
        "<center><h1>1-ab: Introduction to Neural Networks</h1></center>\n",
        "\n",
        "<center><h2><a href=\"https://rdfia.github.io/\">Course link</a></h2></center>\n",
        "\n",
        "# Warning :\n",
        "# Do \"File -> Save a copy in Drive\" before you start modifying the notebook, otherwise your modifications will not be saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NfnKy8NB8J5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee3788d-670d-40e4-81f8-7feae37b4746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-21 15:42:53--  https://github.com/rdfia/rdfia.github.io/raw/master/data/2-ab.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/data/2-ab.zip [following]\n",
            "--2023-11-21 15:42:53--  https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/data/2-ab.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13423991 (13M) [application/zip]\n",
            "Saving to: ‘2-ab.zip’\n",
            "\n",
            "2-ab.zip            100%[===================>]  12.80M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-11-21 15:42:54 (115 MB/s) - ‘2-ab.zip’ saved [13423991/13423991]\n",
            "\n",
            "Archive:  2-ab.zip\n",
            "  inflating: ._2-ab                  \n",
            "  inflating: circles.mat             \n",
            "  inflating: ._circles.mat           \n",
            "  inflating: mnist.mat               \n",
            "  inflating: ._mnist.mat             \n",
            "  inflating: .DS_Store               \n",
            "  inflating: ._.DS_Store             \n",
            "--2023-11-21 15:42:54--  https://github.com/rdfia/rdfia.github.io/raw/master/code/2-ab/utils-data.py\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/code/2-ab/utils-data.py [following]\n",
            "--2023-11-21 15:42:54--  https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/code/2-ab/utils-data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3950 (3.9K) [text/plain]\n",
            "Saving to: ‘utils-data.py’\n",
            "\n",
            "utils-data.py       100%[===================>]   3.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-21 15:42:54 (35.5 MB/s) - ‘utils-data.py’ saved [3950/3950]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/rdfia/rdfia.github.io/raw/master/data/2-ab.zip\n",
        "!unzip -j 2-ab.zip\n",
        "!wget https://github.com/rdfia/rdfia.github.io/raw/master/code/2-ab/utils-data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2vQ_LLdx8J5b",
        "outputId": "6228c4df-001c-4319-ed05-dc26a46b1e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-be1d5eaecf5f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# See Note [Global dependencies]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Can only happen for wheel with cuda libs as PYPI deps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import seaborn as sns\n",
        "%run 'utils-data.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48x_ha7f8J5i"
      },
      "source": [
        "# Part 1 : Forward and backward passes \"by hands\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtizX1JV8J5n"
      },
      "outputs": [],
      "source": [
        "def init_params(nx, nh, ny):\n",
        "    \"\"\"\n",
        "    nx, nh, ny: integers\n",
        "    out params: dictionary\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "\n",
        "    # Initialize weights with a normal distribution of mean 0 and standard deviation 0.3\n",
        "    params[\"Wh\"] = torch.randn(nx, nh) * 0.3\n",
        "    params[\"Wy\"] = torch.randn(nh, ny) * 0.3\n",
        "\n",
        "    # Initialize biases with zeros\n",
        "    params[\"bh\"] = torch.zeros(nh)\n",
        "    params[\"by\"] = torch.zeros(ny)\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk-N_Ny67yo-"
      },
      "outputs": [],
      "source": [
        "def forward(params, X):\n",
        "    \"\"\"\n",
        "    params: dictionary\n",
        "    X: tensor of size (n_batch, nx)\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "\n",
        "    # Just for convenience after\n",
        "    outputs[\"X\"] = X\n",
        "\n",
        "    # Linear combination for hidden layer\n",
        "    outputs[\"htilde\"] = X @ params['Wh'] + params['bh']  # XWh + bh\n",
        "    # Activation function for hidden layer\n",
        "    outputs[\"h\"] = torch.tanh(outputs['htilde'])\n",
        "\n",
        "    # Linear combination for output layer\n",
        "    outputs[\"ytilde\"] = outputs[\"h\"] @ params['Wy'] + params['by']  # hWy + by\n",
        "    # Softmax function for output layer to estimate probability distribution\n",
        "    exp_ytilde = torch.exp(outputs[\"ytilde\"])\n",
        "    outputs[\"yhat\"] = exp_ytilde / torch.sum(exp_ytilde, dim=1, keepdim=True)  # Normalize along the batch dimension\n",
        "\n",
        "    return outputs['yhat'], outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uB0A2b28NZK"
      },
      "outputs": [],
      "source": [
        "def loss_accuracy(Yhat, Y):\n",
        "    # Preparing cross-entropy (CE)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Convert one-hot encoded labels to single-label format\n",
        "    _, targets = torch.max(Y, 1)\n",
        "\n",
        "    # Applying CE\n",
        "    L = criterion(Yhat, targets)\n",
        "\n",
        "    # Same for predicted values\n",
        "    _, indsYhat = torch.max(Yhat, 1)\n",
        "\n",
        "    # Compute the accuracy\n",
        "    acc = (indsYhat == targets).float().mean()\n",
        "\n",
        "    return L, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWJjdiFe8qi5"
      },
      "outputs": [],
      "source": [
        "def backward(params, outputs, Y):\n",
        "    grads = {}\n",
        "\n",
        "    # Computation of theorical definitions, w.r.t. tensor dimensions\n",
        "    grad_ytilde = outputs['yhat'] - Y\n",
        "\n",
        "    grads[\"Wy\"] = outputs['h'].T @ grad_ytilde\n",
        "\n",
        "    grad_htilde = (grad_ytilde @ params[\"Wy\"].T) * (1 - outputs[\"h\"]**2)\n",
        "\n",
        "    grads[\"Wh\"] = outputs[\"X\"].T @ grad_htilde\n",
        "    grads[\"by\"] = grad_ytilde.sum(axis=0)\n",
        "    grads[\"bh\"] = grad_htilde.sum(axis=0)\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAnsISsW9CnH"
      },
      "outputs": [],
      "source": [
        "def sgd(params, grads, eta):\n",
        "\n",
        "    # Updating parameters according to theorical definitions and using eta as a learning rate\n",
        "    params[\"Wh\"] = params[\"Wh\"] - eta * grads[\"Wh\"]\n",
        "    params[\"Wy\"] = params[\"Wy\"] - eta * grads[\"Wy\"]\n",
        "    params[\"bh\"] = params[\"bh\"] - eta * grads[\"bh\"]\n",
        "    params[\"by\"] = params[\"by\"] - eta * grads[\"by\"]\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hifuW5UFA3DZ"
      },
      "source": [
        "## Global learning procedure \"by hands\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results according to *LR*"
      ],
      "metadata": {
        "id": "RdDXx62-wjim"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RSw6bd0-qUe"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "params = init_params(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "params = init_params(nx, nh, ny)  # Initialize parameters for each batch size\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "        # Forward\n",
        "        Yhat, outputs = forward(params, X)\n",
        "        L, acc = loss_accuracy(Yhat, Y)\n",
        "        # Backward\n",
        "        grads = backward(params, outputs, Y)\n",
        "        # Updating parameters\n",
        "        params = sgd(params, grads, eta)\n",
        "\n",
        "    Yhat_train, _ = forward(params, data.Xtrain)\n",
        "    Yhat_test, _ = forward(params, data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "    Ygrid, _ = forward(params, data.Xgrid)\n",
        "\n",
        "    #title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    #data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain)\n",
        "    curves[3].append(Ltest)\n",
        "\n",
        "plt.title(f\"Learning rate :{eta}, batch size : {Nbatch}\")\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "batch_sizes = [10, 20, 50, 100]\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "learning_rates = [0.01, 0.03, 0.05, 0.1]\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "for Nbatch in batch_sizes:\n",
        "    for eta in learning_rates:\n",
        "        curves = [[],[], [], []]\n",
        "        params = init_params(nx, nh, ny)  # Initialize parameters for each batch size\n",
        "        # epoch\n",
        "        for iteration in range(150):\n",
        "            # permute\n",
        "            perm = np.random.permutation(N)\n",
        "            Xtrain = data.Xtrain[perm, :]\n",
        "            Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "            # batches\n",
        "            for j in range(N // Nbatch):\n",
        "                indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "                X = Xtrain[indsBatch, :]\n",
        "                Y = Ytrain[indsBatch, :]\n",
        "                # Forward\n",
        "                Yhat, outputs = forward(params, X)\n",
        "                L, acc = loss_accuracy(Yhat, Y)\n",
        "                # Backward\n",
        "                grads = backward(params, outputs, Y)\n",
        "                # Updating parameters\n",
        "                params = sgd(params, grads, eta)\n",
        "\n",
        "            Yhat_train, _ = forward(params, data.Xtrain)\n",
        "            Yhat_test, _ = forward(params, data.Xtest)\n",
        "            Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "            Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "            Ygrid, _ = forward(params, data.Xgrid)\n",
        "\n",
        "            #title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "            #data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "            curves[0].append(acctrain)\n",
        "            curves[1].append(acctest)\n",
        "            curves[2].append(Ltrain)\n",
        "            curves[3].append(Ltest)\n",
        "\n",
        "        # Store the results\n",
        "        key = (Nbatch, eta)\n",
        "        results[key] = {\n",
        "            'acc_train': curves[0][-1],  # Last accuracy value for training\n",
        "            'acc_test': curves[1][-1],   # Last accuracy value for testing\n",
        "            'loss_train': curves[2][-1], # Last loss value for training\n",
        "            'loss_test': curves[3][-1]   # Last loss value for testing\n",
        "        }"
      ],
      "metadata": {
        "id": "ujY140_sLMJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare matrices for loss and accuracy\n",
        "loss_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "loss_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "\n",
        "for i, Nbatch in enumerate(batch_sizes):\n",
        "    for j, eta in enumerate(learning_rates):\n",
        "        key = (Nbatch, eta)\n",
        "        loss_train_matrix[i, j] = results[key]['loss_train']\n",
        "        loss_test_matrix[i, j] = results[key]['loss_test']\n",
        "        acc_train_matrix[i, j] = results[key]['acc_train']\n",
        "        acc_test_matrix[i, j] = results[key]['acc_test']\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Heatmaps\n",
        "sns.heatmap(loss_train_matrix, ax=ax[0, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "ax[0, 0].set_xlabel('Learning Rate')\n",
        "ax[0, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(loss_test_matrix, ax=ax[0, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 1].set_title('Testing Loss')\n",
        "ax[0, 1].set_xlabel('Learning Rate')\n",
        "ax[0, 1].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_train_matrix, ax=ax[1, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "ax[1, 0].set_xlabel('Learning Rate')\n",
        "ax[1, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_test_matrix, ax=ax[1, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 1].set_title('Testing Accuracy')\n",
        "ax[1, 1].set_xlabel('Learning Rate')\n",
        "ax[1, 1].set_ylabel('Batch Size')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3AO2vgQENBdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHHH5PL8J54"
      },
      "source": [
        "# Part 2 : Simplification of the backward pass with `torch.autograd`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G4q5zP0CEvB"
      },
      "outputs": [],
      "source": [
        "def init_params(nx, nh, ny):\n",
        "    \"\"\"\n",
        "    nx, nh, ny: integers\n",
        "    out params: dictionnary\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "\n",
        "    # Initialize weights with a normal distribution of mean 0 and standard deviation 0.3 with autograd\n",
        "    params[\"Wh\"] = torch.randn(nx, nh) * 0.3\n",
        "    params[\"Wy\"] = torch.randn(nh, ny) * 0.3\n",
        "    params[\"Wh\"].requires_grad = True\n",
        "    params[\"Wy\"].requires_grad = True\n",
        "\n",
        "\n",
        "    # Initialize biases with zeros with autograd\n",
        "    params[\"bh\"] = torch.zeros(nh, requires_grad=True)\n",
        "    params[\"by\"] = torch.zeros(ny, requires_grad=True)\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0tSjpKCyVB"
      },
      "source": [
        "The function `forward` remains unchanged from previous part.\n",
        "\n",
        "The function `backward` is no longer used because of \"autograd\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA4ycHlfBzCK"
      },
      "outputs": [],
      "source": [
        "def sgd(params, eta):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Updating parameters\n",
        "        params[\"Wh\"] -= eta * params[\"Wh\"].grad\n",
        "        params[\"Wy\"] -= eta * params[\"Wy\"].grad\n",
        "        params[\"bh\"] -= eta * params[\"bh\"].grad\n",
        "        params[\"by\"] -= eta * params[\"by\"].grad\n",
        "\n",
        "        # Reset the gradient accumulators\n",
        "        params['Wh'].grad.zero_()\n",
        "        params['Wy'].grad.zero_()\n",
        "        params['bh'].grad.zero_()\n",
        "        params['by'].grad.zero_()\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjgcmgQpDfOb"
      },
      "source": [
        "## Global learning procedure with autograd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p5oR3EqDea-"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.1 # Modification of eta\n",
        "\n",
        "params = init_params(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # Forward\n",
        "        Yhat, _ = forward(params, X)\n",
        "        L, _ = loss_accuracy(Yhat, Y)\n",
        "\n",
        "        # Backward\n",
        "        L.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        params = sgd(params, eta)\n",
        "\n",
        "\n",
        "    Yhat_train, _ = forward(params, data.Xtrain)\n",
        "    Yhat_test, _ = forward(params, data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "    Ygrid, _ = forward(params, data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title)\n",
        "    # detach() is used to remove the predictions from the computational graph in autograd\n",
        "    data.plot_data_with_grid(Ygrid.detach(), title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain.detach().numpy())\n",
        "    curves[3].append(Ltest.detach().numpy())\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "batch_sizes = [10, 20, 50, 100]\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "learning_rates = [0.01, 0.03, 0.05, 0.1]\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "for Nbatch in batch_sizes:\n",
        "    for eta in learning_rates:\n",
        "        curves = [[],[], [], []]\n",
        "        params = init_params(nx, nh, ny)  # Initialize parameters for each batch size\n",
        "        # epoch\n",
        "        for iteration in range(150):\n",
        "            # permute\n",
        "            perm = np.random.permutation(N)\n",
        "            Xtrain = data.Xtrain[perm, :]\n",
        "            Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "            # batches\n",
        "            for j in range(N // Nbatch):\n",
        "                indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "                X = Xtrain[indsBatch, :]\n",
        "                Y = Ytrain[indsBatch, :]\n",
        "\n",
        "                # Forward\n",
        "                Yhat, _ = forward(params, X)\n",
        "                L, _ = loss_accuracy(Yhat, Y)\n",
        "\n",
        "                # Backward\n",
        "                L.backward()\n",
        "\n",
        "                # Updating parameters\n",
        "                params = sgd(params, eta)\n",
        "\n",
        "            Yhat_train, _ = forward(params, data.Xtrain)\n",
        "            Yhat_test, _ = forward(params, data.Xtest)\n",
        "            Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "            Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "            Ygrid, _ = forward(params, data.Xgrid)\n",
        "\n",
        "            #title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "            #data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "            curves[0].append(acctrain)\n",
        "            curves[1].append(acctest)\n",
        "            curves[2].append(Ltrain)\n",
        "            curves[3].append(Ltest)\n",
        "\n",
        "        # Store the results\n",
        "        key = (Nbatch, eta)\n",
        "        results[key] = {\n",
        "            'acc_train': curves[0][-1],  # Last accuracy value for training\n",
        "            'acc_test': curves[1][-1],   # Last accuracy value for testing\n",
        "            'loss_train': curves[2][-1], # Last loss value for training\n",
        "            'loss_test': curves[3][-1]   # Last loss value for testing\n",
        "        }"
      ],
      "metadata": {
        "id": "gnQQMUpFNxbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare matrices for loss and accuracy\n",
        "loss_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "loss_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "\n",
        "for i, Nbatch in enumerate(batch_sizes):\n",
        "    for j, eta in enumerate(learning_rates):\n",
        "        key = (Nbatch, eta)\n",
        "        loss_train_matrix[i, j] = results[key]['loss_train']\n",
        "        loss_test_matrix[i, j] = results[key]['loss_test']\n",
        "        acc_train_matrix[i, j] = results[key]['acc_train']\n",
        "        acc_test_matrix[i, j] = results[key]['acc_test']\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Heatmaps\n",
        "sns.heatmap(loss_train_matrix, ax=ax[0, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "ax[0, 0].set_xlabel('Learning Rate')\n",
        "ax[0, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(loss_test_matrix, ax=ax[0, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 1].set_title('Testing Loss')\n",
        "ax[0, 1].set_xlabel('Learning Rate')\n",
        "ax[0, 1].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_train_matrix, ax=ax[1, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "ax[1, 0].set_xlabel('Learning Rate')\n",
        "ax[1, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_test_matrix, ax=ax[1, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 1].set_title('Testing Accuracy')\n",
        "ax[1, 1].set_xlabel('Learning Rate')\n",
        "ax[1, 1].set_ylabel('Batch Size')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nAuYn0mFOI7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FV1iss68J6H"
      },
      "source": [
        "# Part 3 : Simplification of the forward pass with `torch.nn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6T5Uq7JEl47"
      },
      "source": [
        "`init_params` and `forward` are replaced by the `init_model` function which defines the network architecture and the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-h4r-FH8J6I"
      },
      "outputs": [],
      "source": [
        "def init_model(nx, nh, ny):\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(nx, nh),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(nh, ny),\n",
        "        torch.nn.Softmax()\n",
        "    )\n",
        "\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    return model, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geE_TI96FXnl"
      },
      "outputs": [],
      "source": [
        "def loss_accuracy(loss, Yhat, Y):\n",
        "    # Convert one-hot encoded labels to single-label format\n",
        "    _, targets = torch.max(Y, 1)\n",
        "\n",
        "    L = loss(Yhat, targets)\n",
        "\n",
        "    # Same for predicted values\n",
        "    _, indsYhat = torch.max(Yhat, 1)\n",
        "\n",
        "    acc = torch.mean((indsYhat == targets).float())\n",
        "\n",
        "    return L, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e93bvFiYGKnA"
      },
      "outputs": [],
      "source": [
        "def sgd(model, eta):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad\n",
        "        model.zero_grad()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOxBMmD4Gxtp"
      },
      "source": [
        "## Global learning procedure with autograd and `torch.nn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hMBmCNvHCLn"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.1 # modification of eta\n",
        "\n",
        "model, loss = init_model(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # Forward\n",
        "        Yhat = model(X)\n",
        "        L = loss(Yhat, Y)\n",
        "\n",
        "        # Backward\n",
        "        L.backward()\n",
        "        model = sgd(model, eta)\n",
        "\n",
        "\n",
        "\n",
        "    Yhat_train = model(data.Xtrain)\n",
        "    Yhat_test = model(data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
        "    Ygrid = model(data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title)\n",
        "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain.detach().numpy())\n",
        "    curves[3].append(Ltest.detach().numpy())\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "batch_sizes = [10, 20, 50, 100]\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "learning_rates = [0.01, 0.03, 0.05, 0.1]\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "for Nbatch in batch_sizes:\n",
        "    for eta in learning_rates:\n",
        "        curves = [[],[], [], []]\n",
        "        params = init_params(nx, nh, ny)  # Initialize parameters for each batch size\n",
        "        # epoch\n",
        "        for iteration in range(150):\n",
        "            # permute\n",
        "            perm = np.random.permutation(N)\n",
        "            Xtrain = data.Xtrain[perm, :]\n",
        "            Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "            # batches\n",
        "            for j in range(N // Nbatch):\n",
        "                indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "                X = Xtrain[indsBatch, :]\n",
        "                Y = Ytrain[indsBatch, :]\n",
        "\n",
        "                # Forward\n",
        "                Yhat = model(X)\n",
        "                L = loss(Yhat, Y)\n",
        "\n",
        "                # Backward\n",
        "                L.backward()\n",
        "                model = sgd(model, eta)\n",
        "\n",
        "            Yhat_train = model(data.Xtrain)\n",
        "            Yhat_test = model(data.Xtest)\n",
        "            Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
        "            Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
        "            Ygrid = model(data.Xgrid)\n",
        "\n",
        "            #title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "            #data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "            curves[0].append(acctrain)\n",
        "            curves[1].append(acctest)\n",
        "            curves[2].append(Ltrain)\n",
        "            curves[3].append(Ltest)\n",
        "\n",
        "        # Store the results\n",
        "        key = (Nbatch, eta)\n",
        "        results[key] = {\n",
        "            'acc_train': curves[0][-1],  # Last accuracy value for training\n",
        "            'acc_test': curves[1][-1],   # Last accuracy value for testing\n",
        "            'loss_train': curves[2][-1], # Last loss value for training\n",
        "            'loss_test': curves[3][-1]   # Last loss value for testing\n",
        "        }"
      ],
      "metadata": {
        "id": "1CJe3r_WObwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare matrices for loss and accuracy\n",
        "loss_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "loss_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "\n",
        "for i, Nbatch in enumerate(batch_sizes):\n",
        "    for j, eta in enumerate(learning_rates):\n",
        "        key = (Nbatch, eta)\n",
        "        loss_train_matrix[i, j] = results[key]['loss_train']\n",
        "        loss_test_matrix[i, j] = results[key]['loss_test']\n",
        "        acc_train_matrix[i, j] = results[key]['acc_train']\n",
        "        acc_test_matrix[i, j] = results[key]['acc_test']\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Heatmaps\n",
        "sns.heatmap(loss_train_matrix, ax=ax[0, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "ax[0, 0].set_xlabel('Learning Rate')\n",
        "ax[0, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(loss_test_matrix, ax=ax[0, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 1].set_title('Testing Loss')\n",
        "ax[0, 1].set_xlabel('Learning Rate')\n",
        "ax[0, 1].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_train_matrix, ax=ax[1, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "ax[1, 0].set_xlabel('Learning Rate')\n",
        "ax[1, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_test_matrix, ax=ax[1, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 1].set_title('Testing Accuracy')\n",
        "ax[1, 1].set_xlabel('Learning Rate')\n",
        "ax[1, 1].set_ylabel('Batch Size')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T15IzhwSOxw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoFSrQNsJCnz"
      },
      "source": [
        "# Part 4 : Simplification of the SGD with `torch.optim`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8WtN9loJPqP"
      },
      "outputs": [],
      "source": [
        "def init_model(nx, nh, ny, eta):\n",
        "\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(nx, nh),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(nh, ny),\n",
        "        torch.nn.Softmax()\n",
        "    )\n",
        "\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
        "\n",
        "    return model, loss, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-0rRzPJYDd"
      },
      "source": [
        "The `sgd` function is replaced by calling the `optim.zero_grad()` before the backward and `optim.step()` after."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q82hCupvJxvV"
      },
      "source": [
        "## Algorithme global d'apprentissage (avec autograd, les couches `torch.nn` et `torch.optim`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9h9nINKJ1LU"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.1 # modification of eta\n",
        "\n",
        "model, loss, optim = init_model(nx, nh, ny, eta)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # Forward\n",
        "        Yhat = model(X)\n",
        "        L = loss(Yhat, Y)\n",
        "\n",
        "        # Backward\n",
        "        optim.zero_grad()\n",
        "        L.backward()\n",
        "        optim.step()\n",
        "\n",
        "\n",
        "    Yhat_train = model(data.Xtrain)\n",
        "    Yhat_test = model(data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
        "    Ygrid = model(data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title)\n",
        "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain.detach().numpy())\n",
        "    curves[3].append(Ltest.detach().numpy())\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "batch_sizes = [10, 20, 50, 100]\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "learning_rates = [0.01, 0.03, 0.05, 0.1]\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "for Nbatch in batch_sizes:\n",
        "    for eta in learning_rates:\n",
        "        curves = [[],[], [], []]\n",
        "        params = init_params(nx, nh, ny)  # Initialize parameters for each batch size\n",
        "        # epoch\n",
        "        for iteration in range(150):\n",
        "            # permute\n",
        "            perm = np.random.permutation(N)\n",
        "            Xtrain = data.Xtrain[perm, :]\n",
        "            Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "            # batches\n",
        "            for j in range(N // Nbatch):\n",
        "                indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "                X = Xtrain[indsBatch, :]\n",
        "                Y = Ytrain[indsBatch, :]\n",
        "\n",
        "                # Forward\n",
        "                Yhat = model(X)\n",
        "                L = loss(Yhat, Y)\n",
        "\n",
        "                # Backward\n",
        "                optim.zero_grad()\n",
        "                L.backward()\n",
        "                optim.step()\n",
        "\n",
        "            Yhat_train = model(data.Xtrain)\n",
        "            Yhat_test = model(data.Xtest)\n",
        "            Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
        "            Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
        "            Ygrid = model(data.Xgrid)\n",
        "\n",
        "            #title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "            #data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "            curves[0].append(acctrain)\n",
        "            curves[1].append(acctest)\n",
        "            curves[2].append(Ltrain)\n",
        "            curves[3].append(Ltest)\n",
        "\n",
        "        # Store the results\n",
        "        key = (Nbatch, eta)\n",
        "        results[key] = {\n",
        "            'acc_train': curves[0][-1],  # Last accuracy value for training\n",
        "            'acc_test': curves[1][-1],   # Last accuracy value for testing\n",
        "            'loss_train': curves[2][-1], # Last loss value for training\n",
        "            'loss_test': curves[3][-1]   # Last loss value for testing\n",
        "        }"
      ],
      "metadata": {
        "id": "zaALllp3O-Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare matrices for loss and accuracy\n",
        "loss_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "loss_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "\n",
        "for i, Nbatch in enumerate(batch_sizes):\n",
        "    for j, eta in enumerate(learning_rates):\n",
        "        key = (Nbatch, eta)\n",
        "        loss_train_matrix[i, j] = results[key]['loss_train']\n",
        "        loss_test_matrix[i, j] = results[key]['loss_test']\n",
        "        acc_train_matrix[i, j] = results[key]['acc_train']\n",
        "        acc_test_matrix[i, j] = results[key]['acc_test']\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Heatmaps\n",
        "sns.heatmap(loss_train_matrix, ax=ax[0, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "ax[0, 0].set_xlabel('Learning Rate')\n",
        "ax[0, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(loss_test_matrix, ax=ax[0, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 1].set_title('Testing Loss')\n",
        "ax[0, 1].set_xlabel('Learning Rate')\n",
        "ax[0, 1].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_train_matrix, ax=ax[1, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "ax[1, 0].set_xlabel('Learning Rate')\n",
        "ax[1, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_test_matrix, ax=ax[1, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 1].set_title('Testing Accuracy')\n",
        "ax[1, 1].set_xlabel('Learning Rate')\n",
        "ax[1, 1].set_ylabel('Batch Size')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Vp4vX4yPRYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1s4JuOSaZ3"
      },
      "source": [
        "# Part 5 : MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jly9C4FCSzLP"
      },
      "source": [
        "Apply the code from previous part code to the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osrFoEr_Syi7"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = MNISTData()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 100\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 100\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "model, loss, optim = init_model(nx, nh, ny, eta)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # Forward\n",
        "        Yhat = model(X)\n",
        "        L = loss(Yhat, Y)\n",
        "\n",
        "        # Backward\n",
        "        optim.zero_grad()\n",
        "        L.backward()\n",
        "        optim.step()\n",
        "\n",
        "\n",
        "    Yhat_train = model(data.Xtrain)\n",
        "    Yhat_test = model(data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
        "    # Ygrid = model(data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title)\n",
        "    to_show = random.randint(0, data.Ytest.shape[0]-1) # Defining randomly a sample test to see its prediction, ground truth and associated image (for overwatching purposes)\n",
        "    print(\"Predicted value : {}, Ground truth : {}\".format(torch.argmax(Yhat_test[to_show]), torch.argmax(data.Ytest[to_show]))) # the argmax = class to predict (maximum likelihood for predictions)\n",
        "    plt.imshow(data.Xtest[to_show].reshape(28, 28)) # sample test\n",
        "    plt.show()\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain.detach().numpy())\n",
        "    curves[3].append(Ltest.detach().numpy())\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init\n",
        "data = MNISTData()\n",
        "N = data.Xtrain.shape[0]\n",
        "batch_sizes = [10, 20, 50, 100]\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "learning_rates = [0.01, 0.03, 0.05, 0.1]\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "for Nbatch in batch_sizes:\n",
        "    for eta in learning_rates:\n",
        "        curves = [[],[], [], []]\n",
        "        params = init_params(nx, nh, ny)  # Initialize parameters for each batch size\n",
        "        # epoch\n",
        "        for iteration in range(150):\n",
        "            # permute\n",
        "            perm = np.random.permutation(N)\n",
        "            Xtrain = data.Xtrain[perm, :]\n",
        "            Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "            # batches\n",
        "            for j in range(N // Nbatch):\n",
        "                indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "                X = Xtrain[indsBatch, :]\n",
        "                Y = Ytrain[indsBatch, :]\n",
        "\n",
        "                # Forward\n",
        "                Yhat = model(X)\n",
        "                L = loss(Yhat, Y)\n",
        "\n",
        "                # Backward\n",
        "                optim.zero_grad()\n",
        "                L.backward()\n",
        "                optim.step()\n",
        "\n",
        "            Yhat_train = model(data.Xtrain)\n",
        "            Yhat_test = model(data.Xtest)\n",
        "            Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
        "            Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
        "            Ygrid = model(data.Xgrid)\n",
        "\n",
        "            #title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "            #data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "            curves[0].append(acctrain)\n",
        "            curves[1].append(acctest)\n",
        "            curves[2].append(Ltrain)\n",
        "            curves[3].append(Ltest)\n",
        "\n",
        "        # Store the results\n",
        "        key = (Nbatch, eta)\n",
        "        results[key] = {\n",
        "            'acc_train': curves[0][-1],  # Last accuracy value for training\n",
        "            'acc_test': curves[1][-1],   # Last accuracy value for testing\n",
        "            'loss_train': curves[2][-1], # Last loss value for training\n",
        "            'loss_test': curves[3][-1]   # Last loss value for testing\n",
        "        }"
      ],
      "metadata": {
        "id": "t4qg5DEmPP0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare matrices for loss and accuracy\n",
        "loss_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "loss_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_train_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "acc_test_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
        "\n",
        "for i, Nbatch in enumerate(batch_sizes):\n",
        "    for j, eta in enumerate(learning_rates):\n",
        "        key = (Nbatch, eta)\n",
        "        loss_train_matrix[i, j] = results[key]['loss_train']\n",
        "        loss_test_matrix[i, j] = results[key]['loss_test']\n",
        "        acc_train_matrix[i, j] = results[key]['acc_train']\n",
        "        acc_test_matrix[i, j] = results[key]['acc_test']\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Heatmaps\n",
        "sns.heatmap(loss_train_matrix, ax=ax[0, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "ax[0, 0].set_xlabel('Learning Rate')\n",
        "ax[0, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(loss_test_matrix, ax=ax[0, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[0, 1].set_title('Testing Loss')\n",
        "ax[0, 1].set_xlabel('Learning Rate')\n",
        "ax[0, 1].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_train_matrix, ax=ax[1, 0], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "ax[1, 0].set_xlabel('Learning Rate')\n",
        "ax[1, 0].set_ylabel('Batch Size')\n",
        "\n",
        "sns.heatmap(acc_test_matrix, ax=ax[1, 1], annot=True, xticklabels=learning_rates, yticklabels=batch_sizes)\n",
        "ax[1, 1].set_title('Testing Accuracy')\n",
        "ax[1, 1].set_xlabel('Learning Rate')\n",
        "ax[1, 1].set_ylabel('Batch Size')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ucrWSTBYPg06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRoiGbhvmSLO"
      },
      "source": [
        "# Part 6: Bonus: SVM\n",
        "\n",
        "\n",
        "Train a SVM model on the Circles dataset.\n",
        "\n",
        "Ideas :\n",
        "- First try a linear SVM (sklearn.svm.LinearSVC dans scikit-learn). Does it work well ? Why ?\n",
        "- Then try more complex kernels (sklearn.svm.SVC). Which one is the best ? why ?\n",
        "- Does the parameter C of regularization have an impact? Why ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YdWJKjaPFLd"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC, LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWeW8siymR3g"
      },
      "outputs": [],
      "source": [
        "# data\n",
        "data = CirclesData()\n",
        "Xtrain = data.Xtrain.numpy()\n",
        "Ytrain = data.Ytrain[:, 0].numpy()\n",
        "\n",
        "Xgrid = data.Xgrid.numpy()\n",
        "\n",
        "Xtest = data.Xtest.numpy()\n",
        "Ytest = data.Ytest[:, 0].numpy()\n",
        "\n",
        "def plot_svm_predictions(data, predictions):\n",
        "      plt.figure(2)\n",
        "      plt.clf()\n",
        "      plt.imshow(np.reshape(predictions, (40,40)))\n",
        "      plt.plot(data._Xtrain[data._Ytrain[:,0] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,0] == 1,1]*10+20, 'bo', label=\"Train\")\n",
        "      plt.plot(data._Xtrain[data._Ytrain[:,1] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,1] == 1,1]*10+20, 'ro')\n",
        "      plt.plot(data._Xtest[data._Ytest[:,0] == 1,0]*10+20, data._Xtest[data._Ytest[:,0] == 1,1]*10+20, 'b+', label=\"Test\")\n",
        "      plt.plot(data._Xtest[data._Ytest[:,1] == 1,0]*10+20, data._Xtest[data._Ytest[:,1] == 1,1]*10+20, 'r+')\n",
        "      plt.xlim(0,39)\n",
        "      plt.ylim(0,39)\n",
        "      plt.clim(0.3,0.7)\n",
        "      plt.draw()\n",
        "      plt.pause(1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-qMCxXPJE9"
      },
      "source": [
        "## Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1xcE6zbmXU1"
      },
      "outputs": [],
      "source": [
        "# Linear SVC parameters and fitting\n",
        "\n",
        "l_svc = LinearSVC(random_state=42)\n",
        "l_svc.fit(Xtrain, Ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgLl7B_3mbOs"
      },
      "outputs": [],
      "source": [
        "## Print results\n",
        "\n",
        "Ytest_pred = l_svc.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
        "Ygrid_pred = l_svc.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wyeYbCaPhHu"
      },
      "source": [
        "Does not work very well because LinearSVC is optimal for linear binary classification. Here, we face to a non linear problem (non separable 2D data with a straight line), we can observe it easily because data is dispensed in a circle shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X70xrdAbPyee"
      },
      "source": [
        "## Classical SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjL8v8YBP41w"
      },
      "outputs": [],
      "source": [
        "# SVC parameters and fitting\n",
        "\n",
        "svc = SVC(random_state=42)\n",
        "svc.fit(Xtrain, Ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT0gK5GzRUDG"
      },
      "outputs": [],
      "source": [
        "## Print results\n",
        "\n",
        "Ytest_pred = svc.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
        "Ygrid_pred = svc.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dyis-HARfdu"
      },
      "outputs": [],
      "source": [
        "# Faire deux ou trois autres SVC pour comparer puis moduler la régularisation et expliquer comme dans l'énoncé\n",
        "\n",
        "svc = SVC(kernel='poly', random_state=42)\n",
        "svc.fit(Xtrain, Ytrain)\n",
        "\n",
        "Ytest_pred = svc.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
        "Ygrid_pred = svc.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbQnrVVxDD5r"
      },
      "outputs": [],
      "source": [
        "svc = SVC(kernel='sigmoid', random_state=42)\n",
        "svc.fit(Xtrain, Ytrain)\n",
        "\n",
        "Ytest_pred = svc.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
        "Ygrid_pred = svc.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzgUtAWGDi90"
      },
      "outputs": [],
      "source": [
        "accs = []\n",
        "for reg in range(1, 100):\n",
        "    svc = SVC(C=reg, random_state=42)\n",
        "    svc.fit(Xtrain, Ytrain)\n",
        "\n",
        "    Ytest_pred = svc.predict(Xtest)\n",
        "    accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "    accs.append(accuracy)\n",
        "\n",
        "plt.plot(accs)\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel('Regularization parameter (C)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHpJbI8HFT36"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}